# ETL-Spotify | Data Engineering Project

![Arquitectura](etl-arquitectura.png)

## DescripciÃ³n del Proyecto

Repositorio para el proyecto final del curso Data Engineering en [CODERHOUSE](https://www.coderhouse.com/).

Este proyecto tiene como propÃ³sito llevar a cabo un proceso ETL (Extract, Transform, Load) utilizando datos extraÃ­dos de la WEB API de [Spotify](https://developer.spotify.com/documentation/web-api/tutorials/getting-started). A travÃ©s de este proceso, se extraerÃ¡ informaciÃ³n relevante del catÃ¡logo de Spotify sobre el top 10 de las mejores canciones de un artista por paÃ­s. Luego, se realizarÃ¡ una transformaciÃ³n de los datos para adecuarlos al formato necesario y finalmente se cargarÃ¡n en AWS Redshift, una base de datos en la nube de Amazon.

### AutomatizaciÃ³n con Apache Airflow

Todo el flujo de trabajo ETL se encuentra automatizado mediante el uso de Apache Airflow, una plataforma de orquestaciÃ³n de tareas. Airflow permite programar, monitorear y ejecutar las diferentes etapas del proceso ETL de manera eficiente y confiable. AdemÃ¡s, en caso de que alguna de las tareas falle, se ha configurado un sistema de alertas por correo electrÃ³nico para notificar a los responsables y tomar acciones rÃ¡pidas para solucionar cualquier problema.

### Objetivo

El objetivo principal de este proyecto es proporcionar una soluciÃ³n automatizada para obtener y analizar datos del catÃ¡logo de Spotify. Con el proceso ETL implementado, podremos obtener informaciÃ³n valiosa sobre las mejores canciones de un artista en diferentes paÃ­ses, lo que puede ser Ãºtil para la toma de decisiones y el anÃ¡lisis de tendencias musicales.

## Tabla de Contenidos

-   [DescripciÃ³n del Proyecto](#descripciÃ³n-del-proyecto)
-   [Requisitos](#requisitos)
-   [InstalaciÃ³n](#instalaciÃ³n)
-   [Estructura del Proyecto](#estructura-del-proyecto)
-   [Licencia](#licencia)

## Requisitos

1. Registrarse en la API de [Spotify](https://developer.spotify.com/documentation/web-api/tutorials/getting-started) siguiendo los pasos que te proporciona. Con esto obtendremos los siguientes datos:
    - CLIENT_ID
    - CLIENT_SECRECT
2. Tener instalado [Docker](https://www.docker.com/) en tu laptop.
3. Tener una cuenta en AWS o reemplazar las conexiones por una DB en localhost, por ejemplo, PostgreSQL.
4. Generar una contraseÃ±a para aplicaciones de terceros en [Gmail](https://support.google.com/mail/answer/185833?hl=es-419&ref_topic=3394217&sjid=13457642915339293739-SA)

## InstalaciÃ³n

1. Crear las siguientes carpetas a la misma altura del `docker-compose.yml`.
```bash
mkdir logs
mkdir plugins
mkdir postgres_data
```
2. Crear un archivo con variables de entorno llamado `.env` ubicado a la misma altura que el `docker-compose.yml`. Cuyo contenido sea:
```bash
REDSHIFT_HOST=... # YOUR_REDSHIFT_HOST
REDSHIFT_PORT=... # YOUR_REDSHIFT_PORT
REDSHIFT_DB=... # YOUR_REDSHIFT_DB
REDSHIFT_USER=... # YOUR_REDSHIFT_USER
REDSHIFT_SCHEMA=... # YOUR_REDSHIFT_SCHEMA
REDSHIFT_PASSWORD=... # YOUR_REDSHIFT_PASSWORD
REDSHIFT_URL="jdbc:postgresql://${REDSHIFT_HOST}:${REDSHIFT_PORT}/${REDSHIFT_DB}?user=${REDSHIFT_USER}&password=${REDSHIFT_PASSWORD}"
DRIVER_PATH=/tmp/drivers/postgresql-42.5.2.jar
SPOTIFY_CLIENT_ID="YOUR_CLIENT_ID"
SPOTIFY_CLIENT_SECRET="YOUR_CLIENT_SECRET"
```
3. Descargar las imagenes de Airflow y Spark. En caso de error al descargar las imagenes, debe hacer un login en DockerHub.
```bash
docker pull lucastrubiano/airflow:airflow_2_6_2
docker pull lucastrubiano/spark:spark_3_4_1
```
4. Las imagenes fueron generadas a partir de los Dockerfiles ubicados en `docker_images/`. Si se desea generar las imagenes nuevamente, ejecutar los comandos que estÃ¡n en los Dockerfiles.
5. Ejecutar el siguiente comando para levantar los servicios de Airflow y Spark.
```bash
docker-compose up --build
```
6. Una vez que los servicios estÃ©n levantados, ingresar a Airflow en `http://localhost:8080/`.
7. En la pestaÃ±a `Admin -> Connections` crear una nueva conexiÃ³n con los siguientes datos para Redshift:
    * Conn Id: `redshift_default`
    * Conn Type: `Amazon Redshift`
    * Host: `host de redshift`
    * Database: `base de datos de redshift`
    * Schema: `esquema de redshift`
    * User: `usuario de redshift`
    * Password: `contraseÃ±a de redshift`
    * Port: `5439`
8. En la pestaÃ±a `Admin -> Connections` crear una nueva conexiÃ³n con los siguientes datos para Spark:
    * Conn Id: `spark_default`
    * Conn Type: `Spark`
    * Host: `spark://spark`
    * Port: `7077`
    * Extra: `{"queue": "default"}`
9. En la pestaÃ±a `Admin -> Variables` crear una nueva variable con los siguientes datos:
    * Key: `driver_class_path`
    * Value: `/tmp/drivers/postgresql-42.5.2.jar`
10. En la pestaÃ±a `Admin -> Variables` crear una nueva variable con los siguientes datos:
    * Key: `spark_scripts_dir`
    * Value: `/opt/airflow/scripts`
11. En la pestaÃ±a `Admin -> Variables` crear las nuevas variables con los siguientes datos:
    * Key: `SMTP_EMAIL_FROM`
    * Value: `your_email_from@example.com`
    * Key: `SMTP_EMAIL_PASSWORD`
    * Value: `your_smtp_password`
    * Key: `SMTP_EMAIL_TO`
    * Value: `your_email_to@example.com`
12. Ejecutar el DAG `etl_spotify`.

## Estructura del Proyecto

    ğŸ“dags
        etl_spotify.py
    ğŸ“docker_images
        ğŸ“airflow
            Dockerfile
            requirements.txt
        ğŸ“spark
            Dockerfile
    ğŸ“plugins
    ğŸ“logs
    ğŸ“postgres_data
    ğŸ“scripts
        commons.py
        ETL_Spotify.py
        helpers.py
    .gitignore
    .env
    docker-compose.yml
    README.md

### Tabla: popular_songs

Esta tabla almacena informaciÃ³n sobre canciones populares. A continuaciÃ³n se muestra la descripciÃ³n de sus columnas:

| Columna | Tipo de dato | DescripciÃ³n |  
|  -------------- | ------------ | ---------------------------------------|  
| id_song |  VARCHAR(250)  | Identificador Ãºnico de la canciÃ³n. |  
| song_name |  VARCHAR(250) | TÃ­tulo de la canciÃ³n. |  
| artist |  VARCHAR(250) | Nombre del artista o banda. |
| album  |  VARCHAR(150) | Nombre del Ã¡lbum. |
| popularity | INTEGER | Popularidad de la canciÃ³n. |
| duration_ms | INTEGER  | DuraciÃ³n de la canciÃ³n en milisegundos. |
| song_link  | VARCHAR(250)  | Link de la canciÃ³n. |
| country_code  | VARCHAR(2)  | CÃ³digo del paÃ­s donde se escucha la canciÃ³n. |
| timestamp_  | TIMESTAMP  | Marca de tiempo en que se ejecuta el etl. |
| alternate_key  | VARCHAR(255) | Identificador alternativo usado como PRIMARY KEY |


## Licencia
Este proyecto se encuentra bajo la [Licencia MIT](LICENSE.txt)
    

## Contacto
Hola ğŸ‘‹ Â¿CÃ³mo estÃ¡s? Si te gustÃ³ este proyecto y quieres saber mÃ¡s, puedes contactarme por algunas de estas redes ğŸ™‚

âœ‰ lautaropoletto@gmail.com

ğŸ‘¨â€ğŸ’» https://github.com/lpoletto

ğŸ™‹â€â™‚ï¸ https://www.linkedin.com/in/lautaro-poletto/ 


## CrÃ©ditos

Algunos de los recursos utilizados en el proyecto fueron creados por el Profesor Lucas Trubiano. ğŸ‘‰ https://github.com/lucastrubiano